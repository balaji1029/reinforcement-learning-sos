\section{Finite Markov Decision Processes}

This chapter introduces the concept of a Markov Decision Process (MDP) and its finite variant. We will define the MDP and its components, and discuss the optimal policy and value function. We will also introduce the Bellman equations and the value iteration algorithm, which are used to find the optimal policy and value function.

\subsection{The Agent-Environment Interface}

\subsection{Goals and Rewards}

\subsection{Returns and Episodes}

\subsection{Unified Notation for Episodic and Continuing Tasks}

\subsection{Optimality and Approximation}